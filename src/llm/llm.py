import asyncio
import os

# import psycopg2
from langchain.embeddings.base import Embeddings
from langchain.messages import (AIMessage, AIMessageChunk, HumanMessage,
                                SystemMessage, ToolMessage)
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai.chat_models import ChatGoogleGenerativeAIError
from langchain_huggingface import HuggingFaceEndpointEmbeddings
# from langchain_ollama import OllamaEmbeddings
from langchain_openai import ChatOpenAI
from langchain_postgres.vectorstores import DistanceStrategy, PGVector
from openai import APIConnectionError, OpenAI

from src.config.constant import (LM_STUDIO_IP, PG_COLLECTION, SYSTEM_PROMPT,
                                 TEI_URL)
from src.database import postgreSQL_conn as pgc
from src.rag.tools import create_few_game_rag_tool

# EMBEDDING_MODEL, OLLAMA_LOCAL, OLLAMA_URL, PROJECT_ROOT


"""
å»ºç«‹é€£ç·š
"""
# å»ºç«‹embeddingé€£ç·š
# embeddings = OllamaEmbeddings(
#     model=EMBEDDING_MODEL,
#     base_url=OLLAMA_URL
# )

embeddings = HuggingFaceEndpointEmbeddings(
    model=TEI_URL,
)

# è¼‰å…¥å‘é‡è³‡æ–™åº«
pg_url = pgc.connect_to_pgSQL()
vector_store = PGVector(
    embeddings=embeddings,
    collection_name=PG_COLLECTION,
    connection=pg_url,
    use_jsonb=True,
    distance_strategy=DistanceStrategy.COSINE
)


# å»ºç«‹embeddingé¡åˆ¥
class LmStudioEmbeddings(Embeddings):
    def __init__(self, model_name, url):
        self.model_name = model_name
        self.url = url
        self.client = OpenAI(base_url=url, api_key="lm-studio")

    def embed_query(self, text: str):
        response = self.client.embeddings.create(
            input=text, model=self.model_name)
        return response.data[0].embedding

    def embed_documents(self, texts: list[str]):
        # å›å‚³å¤šå€‹æ–‡ä»¶çš„ embedding
        response = self.client.embeddings.create(
            input=texts, model=self.model_name)
        return [x.embedding for x in response.data]
        # return [self.model.encode(t).tolist() for t in texts]


"""
é¸æ“‡ä¸»è¦LLM
"""


def get_llm(model_option: str):
    """æ ¹æ“šå‰ç«¯é¸æ“‡å›å‚³å°æ‡‰çš„ LLM å¯¦ä¾‹"""
    if "local/Gemma 3 12B" in model_option:
        return ChatOpenAI(
            model='gemma-3-12b-it',
            openai_api_key="not-needed",
            openai_api_base=LM_STUDIO_IP
        )
    elif "free/Gemini 3 flash" in model_option:
        return ChatGoogleGenerativeAI(
            model='gemini-3-flash-preview',
            google_api_key=os.getenv("GOOGLE_API")
        )
    elif "price/Gemini 3 flash" in model_option:
        return ChatGoogleGenerativeAI(
            model='gemini-3-flash-preview',
            google_api_key=os.getenv("GOOGLE_API_PRICE")
        )
    elif "price/ChatGPT 4o mini" in model_option:
        return ChatOpenAI(
            model='gpt-4o-mini',
            openai_api_key=os.getenv("OPENAI_API")
        )
    return None


def init_bot(model_option: str):
    llm = get_llm(model_option)
    few_game_rag = create_few_game_rag_tool(vector_store)
    tools = [few_game_rag]
    return stream_chat_bot(llm, tools)


"""
å®šç¾©é¡åˆ¥åŠæµç¨‹
"""


class stream_chat_bot:
    def __init__(self, llm, tools):
        self.llm = llm
        # åˆå§‹åŒ–å°è©±æ©Ÿå™¨äººï¼Œå‚³å…¥ LLM èˆ‡å¯ç”¨å·¥å…·åˆ—è¡¨
        self.tools = tools
        # å»ºç«‹å·¥å…·åç¨±å°æ‡‰è¡¨ï¼Œç”¨æ–¼å¾ŒçºŒå‹•æ…‹å‘¼å«
        self.tool_map = {tool.name: tool for tool in tools}

        # å°‡ LLM ç¶å®šï¼ˆbindï¼‰å·¥å…·ï¼Œä½¿å…¶å…·å‚™è‡ªå‹•å‘¼å«å·¥å…·çš„èƒ½åŠ›
        self.llm_with_tools = llm.bind_tools(tools)

        # ç³»çµ±æç¤ºè©ï¼ˆSystem Promptï¼‰ï¼Œç”¨ä¾†è¨­å®š LLM çš„è§’è‰²èˆ‡è¡Œç‚º
        self.system_prompt_content = SYSTEM_PROMPT
        # åˆå§‹åŒ–è¨Šæ¯åˆ—è¡¨ï¼Œç¬¬ä¸€æ¢è¨Šæ¯æ˜¯ç³»çµ±æŒ‡ä»¤
        self.message = [SystemMessage(self.system_prompt_content)]

        # å°‡ LLM çš„å›æ‡‰è§£æç‚ºç´”æ–‡å­—æ ¼å¼çš„å·¥å…·
        self.str_parser = StrOutputParser()

    def _get_clean_history_for_auxiliary_llm(self, messages):
        """
        ç‚ºè¼”åŠ© LLM å‡½å¼å»ºç«‹ä¹¾æ·¨çš„å°è©±æ­·å²ï¼Œ
        ç§»é™¤ tool_calls ç›¸é—œå…§å®¹ä»¥é¿å…é•å Gemini API é †åºè¦å‰‡ã€‚

        Gemini API è¦æ±‚ï¼šTool Call å¿…é ˆç·Šæ¥åœ¨ User è¨Šæ¯æˆ– Function Response ä¹‹å¾Œã€‚
        å› æ­¤è¼”åŠ©å‡½å¼ï¼ˆå¦‚ rephraseã€summarizeï¼‰ä¸æ‡‰å‚³å…¥åŒ…å« tool_calls çš„å°è©±æ­·å²ã€‚
        """
        clean_messages = []
        for msg in messages:
            if isinstance(msg, (AIMessage, AIMessageChunk)):
                # åªä¿ç•™ç´”æ–‡å­—å…§å®¹ï¼Œç§»é™¤ tool_calls
                if msg.content:
                    clean_messages.append(AIMessage(content=msg.content))
            elif isinstance(msg, ToolMessage):
                # è·³é ToolMessageï¼Œå› ç‚ºè¼”åŠ©å‡½å¼ä¸éœ€è¦å·¥å…·åŸ·è¡Œçµæœ
                continue
            else:
                # HumanMessage, SystemMessage ç›´æ¥ä¿ç•™
                clean_messages.append(msg)
        return clean_messages

    def _rephrase_query(self, user_input):
        """
        ä¸­é–“å±¤ LLMï¼šå°‡ä½¿ç”¨è€…åŸå§‹è¼¸å…¥è½‰æ›ç‚ºæ›´ç²¾æº–çš„æŸ¥è©¢èªå¥ã€‚
        """
        rephrase_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€å€‹æå•å„ªåŒ–å°ˆå®¶ã€‚è«‹åˆ†æä½¿ç”¨è€…çš„è¼¸å…¥èˆ‡å°è©±æ­·å²ï¼Œ
            å°‡å…¶è½‰æ›ç‚ºä¸€å€‹ã€ç¨ç«‹ã€å®Œæ•´ã€ç²¾æº–ä¸”ç°¡æ½”ã€çš„å•é¡Œï¼Œä»¥ä¾¿è®“å¾ŒçºŒçš„æœå°‹ç³»çµ±èƒ½ç²¾ç¢ºåŸ·è¡Œã€‚

            è¦å‰‡ï¼š
            1. ä¿ç•™æ‰€æœ‰é—œéµè³‡è¨Šï¼ˆå¦‚ï¼šéŠæˆ²åç¨±ã€æ—¥æœŸã€ç‰¹å®šè¡“èªï¼‰ã€‚
            2. ä¿®å¾©éŒ¯å­—æˆ–èªæ„ä¸æ˜ä¹‹è™•ã€‚
            3. å¦‚æœä½¿ç”¨è€…ä½¿ç”¨äº†ä»£åè©ï¼ˆå¦‚ï¼šä»–ã€é€™ä»¶äº‹ï¼‰ï¼Œè«‹æ ¹æ“šæ­·å²ç´€éŒ„æ›¿æ›æˆå…·é«”å…§å®¹ã€‚
            4. ç›´æ¥è¼¸å‡ºå„ªåŒ–å¾Œçš„æå•æ–‡å­—ï¼Œä¸è¦åŒ…å«é¡å¤–çš„è§£é‡‹ã€‚"""),
            # å‚³å…¥éƒ¨åˆ†æ­·å²ç´€éŒ„å¢åŠ ä¸Šä¸‹æ–‡ç†è§£åŠ›
            ("placeholder", "{history}"),
            ("human", "{input}")
        ])

        # ä½¿ç”¨åŸå§‹ LLM é€²è¡Œå¿«é€Ÿè½‰æ›
        rephrase_chain = rephrase_prompt | self.llm | self.str_parser

        # å–æœ€è¿‘çš„ 3 æ¢ç´€éŒ„ä½œç‚ºåƒè€ƒï¼Œä¸¦æ¸…ç† tool_calls ç›¸é—œå…§å®¹
        raw_history = self.message[-3:] if len(self.message) > 1 else []
        history_context = self._get_clean_history_for_auxiliary_llm(
            raw_history)

        refined_query = rephrase_chain.invoke({
            "history": history_context,
            "input": user_input
        })

        # é™åˆ¶æŸ¥è©¢é•·åº¦ï¼Œé˜²æ­¢ HuggingFaceEndpointEmbeddings è¿”å› 413 éŒ¯èª¤
        max_query_length = 500
        if len(refined_query) > max_query_length:
            refined_query = refined_query[:max_query_length]

        return refined_query

    def _summarize_history(self):
        """
        åŸ·è¡Œæ‘˜è¦é‚è¼¯ï¼šä¿ç•™ System Prompt èˆ‡æœ€æ–°çš„ 2 æ¢è¨Šæ¯ï¼Œ
        å°‡å…¶é¤˜çš„æ­·å²ç´€éŒ„å£“ç¸®æˆä¸€æ®µæ‘˜è¦ã€‚
        """
        if not hasattr(self, 'system_prompt_content'):
            self.system_prompt_content = SYSTEM_PROMPT

        if len(self.message) <= 3:
            return

        keep_latest = 2
        to_summarize = self.message[1:-keep_latest]
        recent_messages = self.message[-keep_latest:]

        # æ¸…ç†è¦æ‘˜è¦çš„è¨Šæ¯ï¼Œç§»é™¤ tool_calls ç›¸é—œå…§å®¹
        clean_to_summarize = self._get_clean_history_for_auxiliary_llm(
            to_summarize)

        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å°è©±ç§˜æ›¸ã€‚è«‹å°‡ä¸‹æ–¹çš„å°è©±ç´€éŒ„ç²¾ç°¡å£“ç¸®ï¼Œä¿ç•™æ ¸å¿ƒé‡é»ï¼Œæ¸›å°‘ç´„ 30% ç¸½é•·åº¦ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡æ’°å¯«ã€‚"),
            ("placeholder", "{content}")
        ])

        summary_chain = summary_prompt | self.llm | self.str_parser
        summary_text = summary_chain.invoke({"content": clean_to_summarize})

        # é‡å»ºè¨Šæ¯åˆ—è¡¨æ™‚ï¼Œä¹Ÿæ¸…ç†æœ€è¿‘çš„è¨Šæ¯
        clean_recent_messages = self._get_clean_history_for_auxiliary_llm(
            recent_messages)

        self.message = [
            SystemMessage(content=self.system_prompt_content),
            HumanMessage(content=f"é€™æ˜¯å…ˆå‰çš„å°è©±æ‘˜è¦ï¼š{summary_text}"),
            *clean_recent_messages
        ]
        print(f"\nâœ¨ [ç³»çµ±é€šçŸ¥]: æ­·å²ç´€éŒ„å·²ç²¾ç°¡å®Œæˆã€‚")

    def chat_generator(self, text, display_data=False):
        """
        ä¸»å°è©±ç”Ÿæˆå‡½å¼ï¼ˆç”Ÿæˆå™¨å½¢å¼ï¼‰ã€‚
        é€æ­¥åŸ·è¡Œ LLM å›æ‡‰èˆ‡å·¥å…·èª¿ç”¨ï¼Œä¸¦å³æ™‚å›å‚³æ¯ä¸€æ­¥çš„çµæœã€‚
        """
        try:
            # è‹¥å°è©±ç´€éŒ„è¶…é 3 è¼ªï¼ˆç´„ 8 å‰‡è¨Šæ¯ï¼‰ï¼Œé€²è¡Œæ‘˜è¦
            if len(self.message) > 8:
                self._summarize_history()

            # é€²è¡Œå•é¡Œè½‰è­¯
            refined_text = self._rephrase_query(text)

            # å°‡è½‰å½¹å…§å®¹åŠ å…¥è¨Šæ¯åˆ—è¡¨
            self.message.append(HumanMessage(refined_text))

            while True:
                # å‘¼å« LLMï¼Œå‚³å…¥å®Œæ•´è¨Šæ¯æ­·å²
                print(f"ğŸ”„ [LLM å‘¼å«é–‹å§‹] è¨Šæ¯æ•¸é‡: {len(self.message)}")
                final_ai_message = AIMessageChunk(content="")
                for chunk in self.llm_with_tools.stream(self.message):
                    final_ai_message += chunk
                    if hasattr(chunk, 'content') and chunk.content:
                        # è™•ç† Gemini API å›å‚³ list æ ¼å¼çš„ content
                        content = chunk.content
                        if isinstance(content, list):
                            # æå– list ä¸­çš„ text æ¬„ä½
                            text_parts = [
                                part.get('text', '') for part in content 
                                if isinstance(part, dict) and 'text' in part
                            ]
                            content = ''.join(text_parts)
                        if content:
                            yield content

                print(f"âœ… [LLM å›æ‡‰å®Œæˆ] å…§å®¹é•·åº¦: {len(final_ai_message.content)}, å·¥å…·å‘¼å«æ•¸: {len(final_ai_message.tool_calls)}")
                print(f"ğŸ“ [å›æ‡‰å…§å®¹é è¦½]: {repr(final_ai_message.content[:200]) if final_ai_message.content else '(ç©º)'}")

                response = final_ai_message

                # å°‡ LLM å›æ‡‰åŠ å…¥è¨Šæ¯åˆ—è¡¨
                self.message.append(response)

                # æª¢æŸ¥ LLM æ˜¯å¦è¦æ±‚å‘¼å«å·¥å…·
                is_tools_call = False
                for tool_call in response.tool_calls:
                    is_tools_call = True

                    if display_data:
                        # # é¡¯ç¤º LLM è¦åŸ·è¡Œçš„å·¥å…·åç¨±èˆ‡åƒæ•¸
                        # å®Œæ•´è¨Šæ¯
                        msg = f'[åŸ·è¡Œ]: {tool_call["name"]}({tool_call["args"]})\n-----------\n'
                        yield msg  # ä½¿ç”¨ yield è®“çµæœèƒ½å³æ™‚é¡¯ç¤ºåœ¨è¼¸å‡ºä¸­

                    # å¯¦éš›åŸ·è¡Œå·¥å…·ï¼ˆæ ¹æ“šå·¥å…·åç¨±å‹•æ…‹å‘¼å«å°æ‡‰ç‰©ä»¶ï¼‰
                    if tool_call['name'] in self.tool_map:
                        tool_result = self.tool_map[tool_call['name']].invoke(
                            tool_call['args'])
                    else:
                        tool_result = f"Error: Tool '{tool_call['name']}' not found."

                    if display_data:
                        # # é¡¯ç¤ºå·¥å…·åŸ·è¡Œçµæœ
                        msg = f'[çµæœ]: {tool_result}\n-----------\n'
                        yield msg

                    # å°‡å·¥å…·åŸ·è¡Œçµæœå°è£æˆ ToolMessage å›å‚³çµ¦ LLM
                    # é™åˆ¶å·¥å…·çµæœé•·åº¦ï¼Œé˜²æ­¢è¶…é LLM token é™åˆ¶
                    tool_result_str = str(tool_result)
                    max_tool_result_length = 8000
                    if len(tool_result_str) > max_tool_result_length:
                        tool_result_str = tool_result_str[:max_tool_result_length] + "\n...(çµæœå·²æˆªæ–·)"
                        print(f"âš ï¸ [å·¥å…·çµæœéé•·ï¼Œå·²æˆªæ–·è‡³ {max_tool_result_length} å­—å…ƒ]")

                    tool_message = ToolMessage(
                        content=tool_result_str,               # å·¥å…·åŸ·è¡Œçš„æ–‡å­—çµæœ
                        name=tool_call["name"],                # å·¥å…·åç¨±
                        # å·¥å…·å‘¼å« IDï¼ˆè®“ LLM çŸ¥é“å°æ‡‰å“ªå€‹å‘¼å«ï¼‰
                        tool_call_id=tool_call["id"],
                    )
                    # å°‡å·¥å…·å›å‚³çµæœåŠ å…¥è¨Šæ¯åˆ—è¡¨ï¼Œæä¾› LLM ä¸‹ä¸€è¼ªåƒè€ƒ
                    self.message.append(tool_message)
                    print(f"âœ… [å·¥å…·åŸ·è¡Œå®Œæˆ]: {tool_call['name']}, çµæœé•·åº¦: {len(tool_result_str)} å­—å…ƒ")

                # è‹¥é€™ä¸€è¼ªæ²’æœ‰ä»»ä½•å·¥å…·å‘¼å«ï¼Œè¡¨ç¤º LLM å·²ç¶“ç”Ÿæˆæœ€çµ‚å›è¦†
                if not is_tools_call:
                    break

        except ChatGoogleGenerativeAIError as e:
            # è™•ç† Google Gemini Token è€—ç›¡ (429 RESOURCE_EXHAUSTED)
            if "RESOURCE_EXHAUSTED" in str(e) or "429" in str(e):
                yield "APIé¡åº¦å·²è€—ç›¡ï¼Œè«‹æ›´æ›å…¶ä»–æ¨¡å‹"
                return
            raise e
        except APIConnectionError:
            # è™•ç† Local LLM é€£ç·šå¤±æ•—
            yield "æœ¬åœ°ç«¯ä¼ºæœå™¨ç„¡æ³•é€£æ¥ï¼Œè«‹æ›´æ›å…¶ä»–æ¨¡å‹"
            return

    async def _async_rephrase_query(self, user_input):
        """
        éåŒæ­¥ç‰ˆæœ¬ï¼šå°‡ä½¿ç”¨è€…åŸå§‹è¼¸å…¥è½‰æ›ç‚ºæ›´ç²¾æº–çš„æŸ¥è©¢èªå¥ã€‚
        """
        rephrase_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€å€‹æå•å„ªåŒ–å°ˆå®¶ã€‚è«‹åˆ†æä½¿ç”¨è€…çš„è¼¸å…¥èˆ‡å°è©±æ­·å²ï¼Œ
            å°‡å…¶è½‰æ›ç‚ºä¸€å€‹ã€ç¨ç«‹ã€å®Œæ•´ã€ç²¾æº–ä¸”ç°¡æ½”ã€çš„å•é¡Œï¼Œä»¥ä¾¿è®“å¾ŒçºŒçš„æœå°‹ç³»çµ±èƒ½ç²¾ç¢ºåŸ·è¡Œã€‚

            è¦å‰‡ï¼š
            1. ä¿ç•™æ‰€æœ‰é—œéµè³‡è¨Šï¼ˆå¦‚ï¼šéŠæˆ²åç¨±ã€æ—¥æœŸã€ç‰¹å®šè¡“èªï¼‰ã€‚
            2. ä¿®å¾©éŒ¯å­—æˆ–èªæ„ä¸æ˜ä¹‹è™•ã€‚
            3. å¦‚æœä½¿ç”¨è€…ä½¿ç”¨äº†ä»£åè©ï¼ˆå¦‚ï¼šä»–ã€é€™ä»¶äº‹ï¼‰ï¼Œè«‹æ ¹æ“šæ­·å²ç´€éŒ„æ›¿æ›æˆå…·é«”å…§å®¹ã€‚
            4. ç›´æ¥è¼¸å‡ºå„ªåŒ–å¾Œçš„æå•æ–‡å­—ï¼Œä¸è¦åŒ…å«é¡å¤–çš„è§£é‡‹ã€‚"""),
            ("placeholder", "{history}"),
            ("human", "{input}")
        ])

        rephrase_chain = rephrase_prompt | self.llm | self.str_parser

        raw_history = self.message[-3:] if len(self.message) > 1 else []
        history_context = self._get_clean_history_for_auxiliary_llm(raw_history)

        # ä½¿ç”¨éåŒæ­¥å‘¼å«
        refined_query = await rephrase_chain.ainvoke({
            "history": history_context,
            "input": user_input
        })

        max_query_length = 500
        if len(refined_query) > max_query_length:
            refined_query = refined_query[:max_query_length]

        return refined_query

    async def _async_summarize_history(self):
        """
        éåŒæ­¥ç‰ˆæœ¬ï¼šåŸ·è¡Œæ‘˜è¦é‚è¼¯ï¼Œä¿ç•™ System Prompt èˆ‡æœ€æ–°çš„ 2 æ¢è¨Šæ¯ã€‚
        """
        if not hasattr(self, 'system_prompt_content'):
            self.system_prompt_content = SYSTEM_PROMPT

        if len(self.message) <= 3:
            return

        keep_latest = 2
        to_summarize = self.message[1:-keep_latest]
        recent_messages = self.message[-keep_latest:]

        clean_to_summarize = self._get_clean_history_for_auxiliary_llm(to_summarize)

        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å°è©±ç§˜æ›¸ã€‚è«‹å°‡ä¸‹æ–¹çš„å°è©±ç´€éŒ„ç²¾ç°¡å£“ç¸®ï¼Œä¿ç•™æ ¸å¿ƒé‡é»ï¼Œæ¸›å°‘ç´„ 30% ç¸½é•·åº¦ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡æ’°å¯«ã€‚"),
            ("placeholder", "{content}")
        ])

        summary_chain = summary_prompt | self.llm | self.str_parser
        # ä½¿ç”¨éåŒæ­¥å‘¼å«
        summary_text = await summary_chain.ainvoke({"content": clean_to_summarize})

        clean_recent_messages = self._get_clean_history_for_auxiliary_llm(recent_messages)

        self.message = [
            SystemMessage(content=self.system_prompt_content),
            HumanMessage(content=f"é€™æ˜¯å…ˆå‰çš„å°è©±æ‘˜è¦ï¼š{summary_text}"),
            *clean_recent_messages
        ]
        print(f"\nâœ¨ [ç³»çµ±é€šçŸ¥]: æ­·å²ç´€éŒ„å·²ç²¾ç°¡å®Œæˆã€‚")

    async def async_chat_generator(self, text, display_data=False):
        """
        éåŒæ­¥ç‰ˆæœ¬çš„å°è©±ç”Ÿæˆå‡½å¼ï¼ˆAsync Generatorï¼‰ã€‚
        é¿å…é˜»å¡ Event Loopï¼Œç¢ºä¿ WebSocket å¿ƒè·³æ­£å¸¸ã€‚
        """
        try:
            # è‹¥å°è©±ç´€éŒ„è¶…é 3 è¼ªï¼ˆç´„ 8 å‰‡è¨Šæ¯ï¼‰ï¼Œé€²è¡Œæ‘˜è¦
            if len(self.message) > 8:
                await self._async_summarize_history()

            # é€²è¡Œå•é¡Œè½‰è­¯ï¼ˆéåŒæ­¥ï¼‰
            refined_text = await self._async_rephrase_query(text)

            # å°‡è½‰è­¯å…§å®¹åŠ å…¥è¨Šæ¯åˆ—è¡¨
            self.message.append(HumanMessage(refined_text))

            while True:
                # å‘¼å« LLMï¼Œå‚³å…¥å®Œæ•´è¨Šæ¯æ­·å²ï¼ˆéåŒæ­¥ä¸²æµï¼‰
                print(f"ğŸ”„ [LLM å‘¼å«é–‹å§‹] è¨Šæ¯æ•¸é‡: {len(self.message)}")
                final_ai_message = AIMessageChunk(content="")
                
                # ç”¨æ–¼ç·©è¡å…§å®¹ï¼Œé¿å…åœ¨å·¥å…·å‘¼å«æ™‚é¡¯ç¤ºã€Œæ€è€ƒä¸­ã€çš„æ–‡å­—
                content_buffer = []
                is_tool_turn = False
                stream_started = False
                BUFFER_THRESHOLD = 50  # ç·©è¡å­—å…ƒæ•¸é–¾å€¼

                # ä½¿ç”¨ astream éåŒæ­¥ä¸²æµ
                async for chunk in self.llm_with_tools.astream(self.message):
                    final_ai_message += chunk
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·å‘¼å«
                    if chunk.tool_call_chunks or chunk.tool_calls:
                        is_tool_turn = True
                        # è‹¥ç¢ºå®šæ˜¯å·¥å…·å‘¼å«ï¼Œä¸”å°šæœªé–‹å§‹ä¸²æµé¡¯ç¤ºï¼Œå‰‡æ¸…ç©ºç·©è¡å€ï¼ˆéš±è—æ€è€ƒæ–‡å­—ï¼‰
                        if not stream_started:
                            content_buffer = []
                    
                    # è™•ç†å…§å®¹
                    if hasattr(chunk, 'content') and chunk.content:
                        content = chunk.content
                        if isinstance(content, list):
                            text_parts = [
                                part.get('text', '') for part in content 
                                if isinstance(part, dict) and 'text' in part
                            ]
                            content = ''.join(text_parts)
                        
                        if content:
                            if is_tool_turn:
                                # è‹¥å·²çŸ¥æ˜¯å·¥å…·å‘¼å«å›åˆï¼Œä¸”ä¹‹å‰æ²’é–‹å§‹è¼¸å‡ºï¼Œå‰‡å¿½ç•¥å…§å®¹
                                if not stream_started:
                                    continue
                                else:
                                    # è‹¥å·²ç¶“é–‹å§‹è¼¸å‡ºï¼ˆæ¥µå°‘è¦‹æƒ…æ³ï¼‰ï¼Œåªå¥½ç¹¼çºŒè¼¸å‡º
                                    yield content
                            else:
                                # å°šæœªç¢ºèªæ˜¯å¦ç‚ºå·¥å…·å›åˆ
                                if stream_started:
                                    # å·²ç¶“èªå®šæ˜¯å›ç­”ï¼Œç›´æ¥è¼¸å‡º
                                    yield content
                                else:
                                    # åŠ å…¥ç·©è¡å€
                                    content_buffer.append(content)
                                    current_buffer_len = sum(len(c) for c in content_buffer)
                                    
                                    # è‹¥ç·©è¡å€è¶…éé–¾å€¼ï¼Œèªå®šç‚ºæ­£å¼å›ç­”ï¼Œé–‹å§‹è¼¸å‡º
                                    if current_buffer_len > BUFFER_THRESHOLD:
                                        stream_started = True
                                        for c in content_buffer:
                                            yield c
                                        content_buffer = []

                # è¿´åœˆçµæŸå¾Œï¼Œæª¢æŸ¥æ˜¯å¦é‚„æœ‰ç·©è¡å…§å®¹
                if not is_tool_turn and content_buffer:
                    for c in content_buffer:
                        yield c

                print(f"âœ… [LLM å›æ‡‰å®Œæˆ] å…§å®¹é•·åº¦: {len(final_ai_message.content)}, å·¥å…·å‘¼å«æ•¸: {len(final_ai_message.tool_calls)}")
                print(f"ğŸ“ [å›æ‡‰å…§å®¹é è¦½]: {repr(final_ai_message.content[:200]) if final_ai_message.content else '(ç©º)'}")

                response = final_ai_message
                self.message.append(response)

                # æª¢æŸ¥ LLM æ˜¯å¦è¦æ±‚å‘¼å«å·¥å…·
                is_tools_call = False
                for tool_call in response.tool_calls:
                    is_tools_call = True

                    if display_data:
                        msg = f'[åŸ·è¡Œ]: {tool_call["name"]}({tool_call["args"]})\n-----------\n'
                        yield msg

                    # éåŒæ­¥åŸ·è¡Œå·¥å…·
                    if tool_call['name'] in self.tool_map:
                        tool = self.tool_map[tool_call['name']]
                        # å„ªå…ˆä½¿ç”¨ ainvokeï¼Œè‹¥ä¸æ”¯æ´å‰‡ç”¨ to_thread åŒ…è£
                        if hasattr(tool, 'ainvoke'):
                            tool_result = await tool.ainvoke(tool_call['args'])
                        else:
                            tool_result = await asyncio.to_thread(
                                tool.invoke, tool_call['args']
                            )
                    else:
                        tool_result = f"Error: Tool '{tool_call['name']}' not found."

                    if display_data:
                        msg = f'[çµæœ]: {tool_result}\n-----------\n'
                        yield msg

                    tool_result_str = str(tool_result)
                    max_tool_result_length = 8000
                    if len(tool_result_str) > max_tool_result_length:
                        tool_result_str = tool_result_str[:max_tool_result_length] + "\n...(çµæœå·²æˆªæ–·)"
                        print(f"âš ï¸ [å·¥å…·çµæœéé•·ï¼Œå·²æˆªæ–·è‡³ {max_tool_result_length} å­—å…ƒ]")

                    tool_message = ToolMessage(
                        content=tool_result_str,
                        name=tool_call["name"],
                        tool_call_id=tool_call["id"],
                    )
                    self.message.append(tool_message)
                    print(f"âœ… [å·¥å…·åŸ·è¡Œå®Œæˆ]: {tool_call['name']}, çµæœé•·åº¦: {len(tool_result_str)} å­—å…ƒ")

                if not is_tools_call:
                    break

        except ChatGoogleGenerativeAIError as e:
            if "RESOURCE_EXHAUSTED" in str(e) or "429" in str(e):
                yield "APIé¡åº¦å·²è€—ç›¡ï¼Œè«‹æ›´æ›å…¶ä»–æ¨¡å‹"
                return
            raise e
        except APIConnectionError:
            yield "æœ¬åœ°ç«¯ä¼ºæœå™¨ç„¡æ³•é€£æ¥ï¼Œè«‹æ›´æ›å…¶ä»–æ¨¡å‹"
            return

    def chat(self, text, print_output=False):
        """
        å°è£ç‰ˆå°è©±å‡½å¼ã€‚
        æœƒæ”¶é›† chat_generator çš„æ‰€æœ‰è¼¸å‡ºï¼Œä¸¦çµ„åˆæˆå®Œæ•´çš„å›è¦†å­—ä¸²ã€‚
        """
        msg = ''
        # é€æ­¥å–å¾— chat_generator çš„ç”¢å‡ºå…§å®¹
        for chunk in self.chat_generator(text):
            msg += f"{chunk}"
            if print_output:
                print(chunk, end='')
        # å›å‚³æœ€çµ‚çµ„åˆçš„å°è©±å…§å®¹
        return msg
