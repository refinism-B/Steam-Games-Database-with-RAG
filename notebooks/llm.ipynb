{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51cc398c",
   "metadata": {},
   "source": [
    "## LLM 調用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.messages import (AIMessageChunk, HumanMessage, SystemMessage,\n",
    "                                ToolMessage)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.config.constant import (EMBEDDING_MODEL, OLLAMA_LOCAL, OLLAMA_URL,\n",
    "                                 PG_COLLECTION, PROJECT_ROOT, SYSTEM_PROMPT)\n",
    "from src.database import postgreSQL_conn as pgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da23c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立embedding類別\n",
    "class LmStudioEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name, url):\n",
    "        self.model_name = model_name\n",
    "        self.url = url\n",
    "        self.client = OpenAI(base_url=url, api_key=\"lm-studio\")\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        response = self.client.embeddings.create(input=text,model=self.model_name)\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def embed_documents(self, texts: list[str]):\n",
    "        # 回傳多個文件的 embedding\n",
    "        response = self.client.embeddings.create(input=texts,model=self.model_name)\n",
    "        return [x.embedding for x in response.data]\n",
    "        # return [self.model.encode(t).tolist() for t in texts]\n",
    "\n",
    "\n",
    "class stream_chat_bot:\n",
    "    def __init__(self, llm, tools):\n",
    "        # 初始化對話機器人，傳入 LLM 與可用工具列表\n",
    "        self.tools = tools\n",
    "        # 將 LLM 綁定（bind）工具，使其具備自動呼叫工具的能力\n",
    "        self.llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "        # 系統提示詞（System Prompt），用來設定 LLM 的角色與行為\n",
    "        system_prompt = SYSTEM_PROMPT\n",
    "        # 初始化訊息列表，第一條訊息是系統指令\n",
    "        self.message = [SystemMessage(system_prompt)]\n",
    "\n",
    "        # 將 LLM 的回應解析為純文字格式的工具\n",
    "        self.str_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "    def chat_generator(self, text):\n",
    "        \"\"\"\n",
    "        主對話生成函式（生成器形式）。\n",
    "        逐步執行 LLM 回應與工具調用，並即時回傳每一步的結果。\n",
    "        \"\"\"\n",
    "        # 將使用者的輸入加入訊息列表\n",
    "        self.message.append(HumanMessage(text))\n",
    "\n",
    "        while True:\n",
    "            # 呼叫 LLM，傳入完整訊息歷史\n",
    "            final_ai_message = AIMessageChunk(content=\"\")\n",
    "            for chunk in self.llm_with_tools.stream(self.message):\n",
    "                final_ai_message += chunk\n",
    "                if hasattr(chunk, 'content') and chunk.content:\n",
    "                    yield self.str_parser.invoke(chunk)\n",
    "\n",
    "            response = final_ai_message\n",
    "\n",
    "            # 將 LLM 回應加入訊息列表\n",
    "            self.message.append(response)\n",
    "\n",
    "            # 檢查 LLM 是否要求呼叫工具\n",
    "            is_tools_call = False\n",
    "            for tool_call in response.tool_calls:\n",
    "                is_tools_call = True\n",
    "\n",
    "                # # 顯示 LLM 要執行的工具名稱與參數\n",
    "                msg = f'[執行]: {tool_call[\"name\"]}({tool_call[\"args\"]})\\n-----------\\n' #完整訊息\n",
    "                # # msg = f'[執行]: {tool_call[\"name\"]}()\\n\\n' #簡易訊息\n",
    "                yield msg  # 使用 yield 讓結果能即時顯示在輸出中\n",
    "\n",
    "                # 實際執行工具（根據工具名稱動態呼叫對應物件）\n",
    "                tool_result = globals()[tool_call['name']].invoke(tool_call['args']) \n",
    "\n",
    "                # # 顯示工具執行結果\n",
    "                msg = f'[結果]: {tool_result}\\n-----------\\n'\n",
    "                yield msg\n",
    "\n",
    "                # 將工具執行結果封裝成 ToolMessage 回傳給 LLM\n",
    "                tool_message = ToolMessage(\n",
    "                    content=str(tool_result),          # 工具執行的文字結果\n",
    "                    name=tool_call[\"name\"],            # 工具名稱\n",
    "                    tool_call_id=tool_call[\"id\"],      # 工具呼叫 ID（讓 LLM 知道對應哪個呼叫）\n",
    "                )\n",
    "                # 將工具回傳結果加入訊息列表，提供 LLM 下一輪參考\n",
    "                self.message.append(tool_message)\n",
    "\n",
    "            # 若這一輪沒有任何工具呼叫，表示 LLM 已經生成最終回覆\n",
    "            if is_tools_call == False:\n",
    "                # 將 LLM 回應解析成純文字並輸出\n",
    "                # yield self.str_parser.invoke(response)\n",
    "                return  # 結束對話流程\n",
    "\n",
    "\n",
    "    def chat(self, text, print_output=False):\n",
    "        \"\"\"\n",
    "        封裝版對話函式。\n",
    "        會收集 chat_generator 的所有輸出，並組合成完整的回覆字串。\n",
    "        \"\"\"\n",
    "        msg = ''\n",
    "        # 逐步取得 chat_generator 的產出內容\n",
    "        for chunk in self.chat_generator(text):\n",
    "            msg += f\"{chunk}\"\n",
    "            if print_output:\n",
    "                print(chunk, end='')\n",
    "        # 回傳最終組合的對話內容\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立embedding連線\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=OLLAMA_LOCAL\n",
    ")\n",
    "\n",
    "# 載入向量資料庫\n",
    "pg_url = pgc.connect_to_pgSQL()\n",
    "vector_store = PGVector(\n",
    "        embeddings=embeddings,\n",
    "        collection_name=PG_COLLECTION,\n",
    "        connection=pg_url,\n",
    "        use_jsonb=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76a96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetQAInput(BaseModel):\n",
    "    question: str = Field(description=\"查詢的問題文字\")\n",
    "    k: int = Field(default=5, description=\"要回傳的文件數量\")\n",
    "\n",
    "@tool(args_schema=GetQAInput)\n",
    "def get_qa(question, n=10, k=2):\n",
    "    \"\"\"\n",
    "    根據使用者提出的問題，從向量資料庫中檢索出最相關的 K 筆問答文件。\n",
    "\n",
    "    Args:\n",
    "        question (str): 查詢的問題文字。\n",
    "        n (int): 搜尋子文件的數量。\n",
    "        k (int): 要回傳的文件數量，預設為 2。\n",
    "\n",
    "    Returns:\n",
    "        documents: 檢索到的相似文件列表。\n",
    "    \"\"\"\n",
    "    docs = vector_store.similarity_search(question, k=n)\n",
    "    seen_ids = set()\n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata[\"parent_id\"] not in seen_ids:\n",
    "            seen_ids.add(doc.metadata[\"parent_id\"])\n",
    "            parent_docs = vector_store.similarity_search(query=\"\",k=1,filter={\"doc_id\": doc.metadata[\"parent_id\"]})\n",
    "            if len(parent_docs) > 0:\n",
    "                documents.append(parent_docs[0])\n",
    "\n",
    "    return documents[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0795ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 串接Gemini\n",
    "API_KEY = os.getenv(\"GOOGLE_API\")\n",
    "model_name = 'gemini-3-flash-preview'\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=model_name,\n",
    "    google_api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c058cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 串接LM Studio\n",
    "# model_name = 'gemma-3-12b-it'  # 指定模型名稱，模型名稱會根據下載的模型不同而改變\n",
    "\n",
    "# base_url = 'http://192.168.0.109:1234/v1'  # LM Studio 本地伺服器的URL\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=model_name,\n",
    "#     openai_api_key=\"not-needed\",\n",
    "#     openai_api_base=base_url\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9299b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = stream_chat_bot(llm, [get_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a872933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in bot.chat_generator(\"上古卷軸online的評價如何\"):\n",
    "    print(x, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478978da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in bot.chat_generator(\"請介紹上古卷軸online的遊戲內容\"):\n",
    "    print(x, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7379f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam-games-database-with-rag-1sd27EX4-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
