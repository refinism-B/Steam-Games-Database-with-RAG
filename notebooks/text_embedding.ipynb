{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c27f803",
   "metadata": {},
   "source": [
    "## 文字嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a22e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import tiktoken\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "\n",
    "from src.config.constant import (CHROMA_COLLECTION_NAME, CHROMA_PERSIST_DIR,\n",
    "                                 PROCESSED_DATA_PATH, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958d5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自訂 LM Studio 文字嵌入類別，繼承自 LangChain 的 Embeddings\n",
    "class LmStudioEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name, url):\n",
    "        \"\"\"\n",
    "        初始化 LM Studio Embeddings\n",
    "        :param model_name: 要使用的嵌入模型名稱\n",
    "        :param url: LM Studio 本地或遠端 API 的位址\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.url = url\n",
    "        # 建立 OpenAI 客戶端，連接 LM Studio 的 API\n",
    "        self.client = OpenAI(base_url=url, api_key=\"lm-studio\")\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        \"\"\"\n",
    "        將單筆文字轉換為向量\n",
    "        :param text: 要嵌入的文字\n",
    "        :return: 向量 (list of float)\n",
    "        \"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            input=text,      # 傳入單筆文字\n",
    "            model=self.model_name  # 指定使用的模型\n",
    "        )\n",
    "        # 回傳第一筆 embedding\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def embed_documents(self, texts: list[str]):\n",
    "        \"\"\"\n",
    "        將多筆文字轉換為向量\n",
    "        :param texts: 要嵌入的文字列表\n",
    "        :return: 向量列表，每個元素對應一筆文字\n",
    "        \"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            input=texts,     # 傳入多筆文字\n",
    "            model=self.model_name  # 指定使用的模型\n",
    "        )\n",
    "        # 回傳每筆文字的 embedding\n",
    "        return [x.embedding for x in response.data]\n",
    "\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def connect_to_vector_db(collection_name, embeddings):\n",
    "    return Chroma(\n",
    "        collection_name=CHROMA_COLLECTION_NAME,     # 向量資料表名稱\n",
    "        embedding_function=embeddings,              # 指定embedding模型\n",
    "        persist_directory=CHROMA_PERSIST_DIR        # 向量資料庫存取路徑\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee948a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讀入1951筆資料\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "input_num = 1\n",
    "\n",
    "input_folder = PROJECT_ROOT / PROCESSED_DATA_PATH.format(\"document\")\n",
    "input_file = f\"document_{input_num}.json\"\n",
    "input_path = input_folder / input_file\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "print(f\"讀入{len(data_list)}筆資料\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f520c967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "粗估平均token數: 894.2697334700154\n"
     ]
    }
   ],
   "source": [
    "# 計算單筆資料文本平均字數\n",
    "total = 0\n",
    "for doc in data_list:\n",
    "    word = doc.get(\"context\", \"\")\n",
    "    total += len(word)\n",
    "\n",
    "token_count = (total / len(data_list)) / 4\n",
    "\n",
    "print(f\"粗估平均token數: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc5a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功轉換1951筆Document物件資料！\n"
     ]
    }
   ],
   "source": [
    "# 轉換成Document物件\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc_list = [Document(page_content=d[\"context\"], metadata=d[\"metadata\"]) for d in data_list]\n",
    "\n",
    "print(f\"已成功轉換{len(doc_list)}筆Document物件資料！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67487b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已建立embedding模型連線\n"
     ]
    }
   ],
   "source": [
    "# 載入Embedding模型\n",
    "embedding_model = \"text-embedding-bge-m3\"\n",
    "embedding_url = \"http://192.168.0.109:1234/v1\"\n",
    "\n",
    "embeddings = LmStudioEmbeddings(model_name=embedding_model, url=embedding_url)\n",
    "print(\"已建立embedding模型連線\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "父層文件（原始）數量：1951\n",
      "分段後文件數量：10291\n"
     ]
    }
   ],
   "source": [
    "# 進行文本切割\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=70, length_function=tiktoken_len)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=250, length_function=tiktoken_len)\n",
    "\n",
    "total_docs = []\n",
    "\n",
    "parent_docs = parent_splitter.split_documents(doc_list)         # 父文件不切割，用完整文件列表\n",
    "\n",
    "for doc in parent_docs:                                         # for迴圈父文件列表\n",
    "    parent_num = 1\n",
    "    parent_id = str(doc.metadata.get(\"steam_appid\"))            # 存取steam_appid值\n",
    "    doc.metadata[\"doc_id\"] = parent_id + f\"_p0{str(parent_num)}\"                          # 建立doc_id，值為steam_appid\n",
    "    doc.metadata[\"parent_id\"] = doc.metadata[\"doc_id\"]                      # 建立parent_id，值為steam_appid\n",
    "\n",
    "    split_docs = child_splitter.split_documents([doc])          # 將文件切割成子文件\n",
    "    child_num = 1                                                     # 子文件編號計數\n",
    "    for sdoc in split_docs:                                     # for迴圈遍歷每個切割出來的子文件\n",
    "        sdoc.metadata[\"doc_id\"] = parent_id + f\"_c0{str(num)}\"     # 為子文件加上doc_id（子文件的唯一id值），值為steam_appid加編號\n",
    "        sdoc.metadata[\"parent_id\"] = doc.metadata[\"doc_id\"]                   # 為子文件加上parent_id，值為steam_appid\n",
    "        num += 1                                                # 子文件編號+1\n",
    "    total_docs.append(doc)                                      # 將父文件放入總文件列表\n",
    "    total_docs.extend(split_docs)                               # 將子文件列表也放入總文件列表\n",
    "\n",
    "print(f\"父層文件（原始）數量：{len(parent_docs)}\")\n",
    "print(f\"分段後文件數量：{len(total_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fef3bd",
   "metadata": {},
   "source": [
    "## 存入向量資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce23eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重前資料筆數: 10291, 10291\n",
      "去重後資料筆數: 10287, 10287\n"
     ]
    }
   ],
   "source": [
    "# 輸入向量資料庫前先進行簡單去重複\n",
    "ids = [doc.metadata['doc_id'] for doc in total_docs]\n",
    "\n",
    "# 藉由重新配對doc和ids，將重複值去除\n",
    "unique_data = {doc_id: doc for doc_id, doc in zip(ids, total_docs)}\n",
    "\n",
    "# 取得新的ids和docs\n",
    "unique_ids = list(unique_data.keys())\n",
    "unique_docs = list(unique_data.values())\n",
    "\n",
    "print(f\"去重前資料筆數: {len(total_docs)}, {len(ids)}\")\n",
    "print(f\"去重後資料筆數: {len(unique_docs)}, {len(unique_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb916c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "寫入進度: 100%|██████████| 52/52 [10:26<00:00, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功將10291筆資料存入向量資料庫(C:\\Users\\add41\\Documents\\Data_Engineer\\Project\\Steam-Games-Database-with-RAG\\data\\vector)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立向量資料庫並存入資料\n",
    "vector_store = connect_to_vector_db(collection_name=CHROMA_COLLECTION_NAME, embeddings=embeddings)\n",
    "\n",
    "batch_size = 200\n",
    "total_docs_count = len(total_docs)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(unique_docs), batch_size), desc=\"寫入進度\"):\n",
    "    batch_docs = unique_docs[i : i + batch_size]\n",
    "    batch_ids = unique_ids[i : i + batch_size]\n",
    "\n",
    "    vector_store.add_documents(\n",
    "            documents=batch_docs,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"已成功將{len(total_docs)}筆資料存入向量資料庫({CHROMA_PERSIST_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e89b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = connect_to_vector_db(collection_name=CHROMA_COLLECTION_NAME, embeddings=embeddings)\n",
    "vector_store.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam-games-database-with-rag-1sd27EX4-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
